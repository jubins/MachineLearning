{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Name: Jubin Soni (jas1464)\n",
    "Question#1\n",
    "Machine Learning Homework#3\n",
    "'''\n",
    "\n",
    "# %load tools.py\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import math\n",
    "from unidecode import unidecode\n",
    "\n",
    "spchars = re.compile('\\`|\\~|\\!|\\@|\\#|\\$|\\%|\\^|\\&|\\*|\\(|\\)|\\_|\\+|\\=|\\\\|\\||\\{|\\[|\\]|\\}|\\:|\\;|\\'|\\\"|\\<|\\,|\\>|\\?|\\/|\\.|\\-')\n",
    "\n",
    "# Utility function that does the following to the text:\n",
    "# - Convert to unicode\n",
    "# - Convert to lowercase\n",
    "# - Remove special chars\n",
    "def make_text_parsable(text):\n",
    "    # convert to unicode\n",
    "    text = unidecode(text) #.decode('utf-8', 'ignore'))\n",
    "    # convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # remove special characters\n",
    "    text = spchars.sub(\" \", text)\n",
    "    return(text)\n",
    "\n",
    "#\n",
    "# Tokenize by whitespace. Use the defaultdict(int) whichsets the default \n",
    "# factory to int which makes it  the default dict useful for counting. \n",
    "#\n",
    "def count_words(text, wc=None):\n",
    "    if wc == None:\n",
    "        wc = defaultdict(int)\n",
    "    tokens = text.split(\" \")\n",
    "    for t in tokens:\n",
    "        wc[t] += 1  \n",
    "    return(wc)\n",
    "\n",
    "#\n",
    "# Main function. Opens the file and calls helper functions to parse\n",
    "# Returns the sorted word count\n",
    "#\n",
    "def extract_info(filename):\n",
    "    import json\n",
    "    wc = defaultdict(int)\n",
    "    df = defaultdict(set)\n",
    "    count = 0\n",
    "    with open(filename) as fin:\n",
    "        for line in fin:\n",
    "            count += 1\n",
    "            current = json.loads(line)\n",
    "            text = make_text_parsable(current[\"abstract\"] + \" \" + \\\n",
    "                current[\"description\"] + \" \" + current[\"title\"])\n",
    "            wc = count_words(text, wc)\n",
    "    \n",
    "    sorted_wc = sorted(wc.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section A: Term frequency with unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 1919), ('the', 608), ('and', 540), ('of', 452)]\n",
      "Answer1: Most common word: 'the'.\n"
     ]
    }
   ],
   "source": [
    "# Question 1: Without modifying the code, which is the most common word?\n",
    "dataFile = 'data_file.txt'\n",
    "print extract_info(dataFile)[0:4] #Fetching only top 5 results\n",
    "\n",
    "\n",
    "#Answer1:\n",
    "print \"Answer1: Most common word: 'the'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 231), ('with', 133), ('python', 122), ('time', 113), ('abstract', 83), ('description', 81), ('title', 79), ('learning', 77), ('talk', 75), ('analysis', 61), ('ll', 52), ('more', 51), ('use', 48), ('3', 43), ('pandas', 42), ('machine', 39), ('10', 38), ('models', 37), ('saturday', 36), ('tools', 36), ('analytics', 33), ('code', 33), ('science', 31), ('discuss', 29), ('based', 29), ('spark', 28), ('model', 28), ('new', 27), ('sunday', 27), ('20', 24), ('tutorial', 23), ('learn', 23), ('both', 23), ('work', 21), ('techniques', 21), ('50', 21), ('5', 21), ('web', 21), ('methods', 20), ('1', 20), ('library', 20), ('2', 19), ('performance', 19), ('world', 19), ('coming', 19), ('algorithm', 19), ('software', 19), ('algorithms', 18), ('numpy', 18), ('building', 18), ('soon', 18), ('30', 18), ('40', 18), ('series', 17), ('applications', 17), ('statistical', 17), ('space', 17), ('different', 17), ('4', 17), ('deep', 17), ('friday', 17), ('build', 17), ('libraries', 17), ('examples', 17), ('intel', 16), ('describe', 16), ('large', 16), ('processing', 16), ('users', 16), ('project', 15), ('implementation', 15), ('people', 15), ('language', 15), ('open', 15), ('11', 15), ('most', 15), ('available', 15), ('uses', 15), ('source', 15), ('application', 15), ('explore', 15), ('system', 15), ('way', 14), ('show', 14), ('real', 14), ('framework', 14), ('pipeline', 14), ('problem', 14), ('testing', 13), ('complex', 13), ('introduce', 13), ('scientists', 13), ('goal', 13), ('look', 13), ('9', 13), ('user', 13), ('cloud', 13), ('statistics', 13), ('scikit', 13), ('provide', 12), ('features', 12), ('best', 12), ('quickly', 12), ('basic', 12), ('dask', 12), ('workshops', 12), ('provides', 12), ('fast', 12), ('start', 12), ('interactive', 12), ('results', 12), ('api', 12), ('recommendation', 12), ('pymc', 11), ('end', 11), ('memory', 11), ('training', 11), ('programming', 11), ('take', 11), ('nlp', 11), ('scientific', 11), ('point', 11), ('simple', 11), ('create', 11), ('pydata', 11), ('include', 11), ('powerful', 11), ('need', 11), ('distribution', 11), ('tool', 10), ('approach', 10), ('developers', 10), ('focus', 10), ('finally', 10), ('core', 10), ('trend', 10), ('working', 10), ('brain', 10), ('platform', 10), ('challenges', 10), ('12', 10), ('topics', 10), ('overview', 10), ('engineering', 10), ('driven', 10), ('pysnptools', 10), ('package', 10), ('search', 10), ('experience', 10), ('social', 10), ('research', 10), ('part', 10), ('session', 10), ('community', 10), ('useful', 10), ('scale', 10), ('simulation', 10), ('blaze', 10), ('high', 10), ('including', 10), ('ecosystem', 10), ('estimation', 10), ('ve', 10), ('twitter', 10), ('debris', 9), ('example', 9), ('better', 9), ('scalable', 9), ('method', 9), ('detection', 9), ('identify', 9), ('apache', 9), ('predict', 9), ('pyspark', 9), ('non', 9), ('lunch', 9), ('sponsor', 9), ('predictive', 9), ('future', 9), ('breakfast', 9), ('break', 9), ('without', 9), ('practical', 9), ('important', 9), ('program', 9), ('help', 9), ('systems', 9), ('health', 9), ('introduction', 9), ('derivative', 9), ('cover', 9), ('big', 9), ('game', 9), ('step', 9), ('problems', 9), ('few', 9), ('snacks', 9), ('collection', 9), ('doing', 9), ('related', 9)]\n",
      "Answer2: Most common 3 words: 'data', 'with', 'python'.\n"
     ]
    }
   ],
   "source": [
    "#Question2: Notice that a lot of the more common words don’t give much info since they are what we can stopwords.\n",
    "#Stop words are natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"an\", and similar words.\n",
    "#Now, look at the file called stopwords and modify tools.py so that it excludes stopwords in the counting.\n",
    "#What are the 3 most common words now?\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "stop_words = word_tokenize(open('stopwords.txt').read())\n",
    "data_file = make_text_parsable(open('data_file.txt').read())\n",
    "data_file = word_tokenize(data_file)\n",
    "\n",
    "cleaned_text = filter(lambda x: x not in stop_words, data_file)\n",
    "\n",
    "print (nltk.FreqDist(cleaned_text)).most_common(200)\n",
    "\n",
    "#Answer2:\n",
    "print \"Answer2: Most common 3 words: 'data', 'with', 'python'.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer3: Words that dont add value: 'data', 'with', 'python'.\n"
     ]
    }
   ],
   "source": [
    "#Question3: Given that this is a list of talks about data and python, do you think that there are some words\n",
    "#that don’t add much value? Name 3 of them and add them to the stopwords list\n",
    "\n",
    "#Answer3: If we look at 3 most common words we find that there are words like 'with', 'data', 'python' that do not add much value.\n",
    "#However if we look at 50 most common words we should notice more trivial words: 'with', 'll', '3' that occur more frequenctly. Hence should be removed.\n",
    "\n",
    "stop_words.append('with')\n",
    "stop_words.append(\"data\")\n",
    "stop_words.append(\"python\")\n",
    "stop_words.append(\"abstract\")\n",
    "stop_words.append(\"title\")\n",
    "stop_words.append(\"description\")\n",
    "stop_words.append(\"time\")\n",
    "\n",
    "print \"Answer3: Words that dont add value: 'data', 'with', 'python'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('learning', 77), ('talk', 75), ('analysis', 61), ('ll', 52), ('more', 51), ('use', 48), ('3', 43), ('pandas', 42), ('machine', 39), ('10', 38)]\n",
      "Answer4: Most common word now is 'learning', 'talk', 'analysis'.\n"
     ]
    }
   ],
   "source": [
    "#Question4: Now run your code again with the new list of stopwords.  What’s the most common word now?\n",
    "cleaned_text = filter(lambda x: x not in stop_words, data_file)\n",
    "print (nltk.FreqDist(cleaned_text)).most_common(10)\n",
    "\n",
    "#Answer4: The most common word now is 'time'.\n",
    "\n",
    "print \"Answer4: Most common word now is 'learning', 'talk', 'analysis'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section B: Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'current' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-9732a6d9d51a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFIDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0md\u001b[0m \u001b[1;31m#prints TF-IDF score for all the stop_words for 79 documents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-9732a6d9d51a>\u001b[0m in \u001b[0;36mTFIDF\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mTFIDF\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mIdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mTf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mTFIDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mIdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTFIDF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-9732a6d9d51a>\u001b[0m in \u001b[0;36mTF\u001b[0;34m(word, data_file)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mterm_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmake_text_parsable_mod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mterm_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-afd0da8a8d47>\u001b[0m in \u001b[0;36mmake_text_parsable_mod\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[1;31m# remove special characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_text_parsable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"abstract\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m                 \u001b[0mcurrent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"description\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"title\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspchars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'current' is not defined"
     ]
    }
   ],
   "source": [
    "#Question5: Compute TF-IDF for the corpus. Note that you will need to compute a separate score for TF per document.\n",
    "#Again in our file we have 79 documents\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import math\n",
    "import numpy\n",
    "import json\n",
    "\n",
    "spchars = re.compile('\\`|\\~|\\!|\\@|\\#|\\$|\\%|\\^|\\&|\\*|\\(|\\)|\\_|\\+|\\=|\\\\|\\||\\{|\\[|\\]|\\}|\\:|\\;|\\'|\\\"|\\<|\\,|\\>|\\?|\\/|\\.|\\-')\n",
    "\n",
    "def make_text_parsable_modified(text):\n",
    "    # convert to unicode\n",
    "    text = unidecode(text) #.decode('utf-8', 'ignore'))\n",
    "    # convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # remove special characters\n",
    "    text = spchars.sub(\" \", text)\n",
    "    return(text)\n",
    "\n",
    "\n",
    "stopWordsFile = 'data_file.txt'\n",
    "dataFile = 'data_file.txt'\n",
    "stop_words = word_tokenize(make_text_parsable(open('data_file.txt').read()))\n",
    "#data_file = word_tokenize(make_text_parsable(open('data_file.txt').read()))\n",
    "#print data_file\n",
    "#print stop_words\n",
    "\n",
    "def TF(word, data_file):\n",
    "    tf = []\n",
    "    for document in data_file:\n",
    "        current = json.loads(document)\n",
    "        document = make_text_parsable(current[\"abstract\"] + \" \" + \\\n",
    "                current[\"description\"] + \" \" + current[\"title\"])\n",
    "        if word in document:\n",
    "            term = document.count(word)\n",
    "            term_count = float(len(nltk.word_tokenize(make_text_parsable_mod(document))))\n",
    "            tf.append(term / term_count)\n",
    "        else:\n",
    "            tf.append(0)\n",
    "    return tf\n",
    "\n",
    "def IDF(word, data_file):\n",
    "    total_documents = len(data_file)\n",
    "    if word in data_file:\n",
    "        word_count = float(data_file.count(word))\n",
    "        return math.log(total_documents/word_count)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def TFIDF (word):\n",
    "    Idf = IDF(word, open(dataFile).read())\n",
    "    Tf = TF(word, open(dataFile).readlines())\n",
    "    TFIDF = numpy.array(Tf) * Idf\n",
    "    return max(TFIDF)\n",
    "\n",
    "d = {}\n",
    "for word in stop_words:\n",
    "    d[word] = TFIDF(word)\n",
    "\n",
    "print d #prints TF-IDF score for all the stop_words for 79 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '05', '1', '10', '100', '100x', '10min', '11', '12', '15', '15min', '16', '17k', '2', '20', '200', '2002', '2003', '2014', '2015', '2020', '2040', '20min', '21st', '23', '25', '2d', '3', '30', '30secs', '35', '3d', '4', '40', '45', '5', '50', '55', '5min', '6', '70', '8', '9', 'a', 'abilities', 'ability', 'able', 'about', 'above', 'absence', 'abstract', 'abstraction', 'abstractions', 'abstracts', 'academic', 'academics', 'accelerate', 'accelerating', 'acceleration', 'access', 'accesses', 'accessible', 'accomodate', 'accomplish', 'accordingly', 'account', 'accounted', 'accreditation', 'accuracy', 'accurate', 'achieve', 'achieved', 'across', 'action', 'actionable', 'actions', 'active', 'actively', 'activity', 'actually', 'acyclic', 'ad', 'adapt', 'adding', 'addition', 'additions', 'address', 'addressing', 'adept', 'administrative', 'admissions', 'ads', 'advanced', 'advances', 'advantages', 'advertisers', 'advertising', 'advice', 'advising', 'advocate', 'affair', 'affect', 'affects', 'after', 'afterthought', 'against', 'aggregation', 'aggregations', 'agnostic', 'ahead', 'ai', 'aim', 'aimed', 'aiming', 'airfare', 'alert', 'alexnet', 'algorithm', 'algorithmic', 'algorithms', 'all', 'allow', 'allowing', 'allows', 'alone', 'along', 'alongside', 'already', 'also', 'altercation', 'altercations', 'altering', 'although', 'always', 'amazing', 'ambiguities', 'america', 'amoeba', 'among', 'amount', 'amounts', 'ample', 'an', 'anaconda', 'analogies', 'analyses', 'analysis', 'analyst', 'analysts', 'analytical', 'analytics', 'analyze', 'analyzing', 'and', 'animated', 'anomaly', 'another', 'answer', 'answering', 'anti', 'anticipating', 'antidotes', 'antipatterns', 'any', 'anyone', 'apache', 'api', 'apis', 'app', 'appealing', 'appearing', 'appetite', 'application', 'applications', 'applied', 'applies', 'apply', 'applying', 'appnexus', 'approach', 'approaches', 'approaching', 'appropriate', 'appropriately', 'approximate', 'approximation', 'apps', 'arborist', 'arc', 'architectural', 'architecture', 'are', 'area', 'areas', 'aren', 'arguments', 'arizona', 'army', 'around', 'array', 'arrays', 'art', 'article', 'articles', 'as', 'ask', 'asking', 'aspect', 'aspects', 'assess', 'assessments', 'assign', 'assignment', 'associated', 'association', 'assume', 'assumes', 'assumption', 'assumptions', 'astrophysical', 'asynchronous', 'at', 'atigeo', 'attempt', 'attendees', 'audience', 'augments', 'auto', 'automated', 'automatic', 'automatically', 'automatons', 'availability', 'available', 'average', 'avoid', 'away', 'axons', 'azure', 'b', 'bacardi', 'back', 'backbone', 'backend', 'backends', 'background', 'backgrounds', 'backpropagation', 'backup', 'bad', 'bag', 'bandpass', 'based', 'basic', 'basics', 'basis', 'bayes', 'bayesian', 'bcolz', 'be', 'beats', 'beautiful', 'beautifulsoup', 'became', 'because', 'become', 'becomes', 'becoming', 'bed', 'bee', 'been', 'before', 'began', 'begin', 'beginners', 'beginning', 'begun', 'behavior', 'behavioral', 'behind', 'being', 'believes', 'belong', 'below', 'bench', 'benchmarks', 'benefit', 'benefits', 'best', 'beta', 'better', 'between', 'bias', 'biased', 'bicycle', 'big', 'bike', 'bikes', 'bikeshed', 'billion', 'billions', 'binary', 'bins', 'bioinfomatics', 'biomedical', 'birds', 'birth', 'bit', 'blaze', 'blind', 'block', 'blocked', 'blocking', 'blocks', 'blog', 'boarding', 'boiled', 'bokeh', 'bold', 'bolts', 'book', 'boolean', 'boosting', 'bootstrapping', 'boston', 'bot', 'both', 'bots', 'bottlenecks', 'bottom', 'boundaries', 'bounded', 'bow', 'box', 'brain', 'brains', 'branch', 'brand', 'brands', 'brawn', 'break', 'breakfast', 'breakpoints', 'brief', 'briefly', 'bring', 'bringing', 'brings', 'broad', 'broader', 'browser', 'build', 'building', 'built', 'bunch', 'bundles', 'burden', 'business', 'but', 'buying', 'buzzword', 'by', 'bytecode', 'c', 'caffe', 'cake', 'calculated', 'calculation', 'calculator', 'call', 'called', 'calling', 'calls', 'came', 'can', 'candles', 'capabilities', 'capability', 'capsules', 'card', 'care', 'carefully', 'carlo', 'case', 'cases', 'cast', 'categoricals', 'categories', 'cauchy', 'causal', 'cause', 'causes', 'caveats', 'cdsw', 'cell', 'center', 'centre', 'century', 'certain', 'chain', 'chained', 'chains', 'challenge', 'challenges', 'change', 'changed', 'changes', 'changing', 'characteristics', 'characterize', 'charge', 'charting', 'check', 'cherrypy', 'choices', 'choose', 'choosing', 'chunked', 'churn', 'circumstances', 'city', 'civic', 'clarity', 'class', 'classes', 'classification', 'classifications', 'classify', 'cleaning', 'clear', 'click', 'clicked', 'client', 'clothing', 'cloud', 'clouded', 'clouds', 'clr', 'clrs', 'cluster', 'clustering', 'clusters', 'co', 'cod', 'code', 'coded', 'codem', 'codes', 'coding', 'cognitive', 'collaborative', 'collaborators', 'colleagues', 'collect', 'collected', 'collecting', 'collection', 'collective', 'collision', 'collisions', 'colony', 'com', 'combination', 'combinations', 'combine', 'combined', 'come', 'comes', 'coming', 'command', 'comment', 'commitment', 'commodity', 'common', 'commonly', 'communication', 'communities', 'community', 'commuter', 'companies', 'company', 'compare', 'comparing', 'comparison', 'compartmental', 'compartments', 'competing', 'competitions', 'compiled', 'complement', 'complete', 'completely', 'complex', 'complexities', 'complexity', 'complicated', 'complimentary', 'complying', 'components', 'computation', 'computational', 'computationally', 'computations', 'compute', 'computer', 'computers', 'computes', 'computing', 'concept', 'concepts', 'conceptualization', 'concerned', 'concerns', 'concise', 'concretely', 'conditionals', 'conference', 'configuration', 'confusing', 'confusion', 'connect', 'connected', 'connecting', 'connections', 'connectivity', 'cons', 'consent', 'conserving', 'considerable', 'considered', 'consistent', 'consists', 'constraints', 'constructing', 'construction', 'constructive', 'constructs', 'consumption', 'containers', 'contains', 'content', 'context', 'continuous', 'continuously', 'contrast', 'contributing', 'contributors', 'control', 'controls', 'conversations', 'conversion', 'converters', 'convince', 'convolutional', 'cooperating', 'core', 'cores', 'corner', 'corporate', 'corpus', 'correctly', 'correspondence', 'cosmic', 'cosmology', 'cost', 'costly', 'could', 'counterexamples', 'counterfactual', 'country', 'course', 'courses', 'cover', 'covered', 'cpython', 'create', 'created', 'creating', 'creation', 'credibility', 'critical', 'cross', 'crossvalidation', 'crowded', 'crowdsourcing', 'crown', 'crucially', 'css', 'csv', 'cufflinks', 'culture', 'current', 'currently', 'curricula', 'curriculum', 'customer', 'customers', 'cutting', 'cycles', 'd', 'd3', 'daal', 'damage', 'dark', 'dashboard', 'dashboards', 'dask', 'data', 'database', 'dataframe', 'dataframes', 'dataset', 'datasets', 'dato', 'day', 'days', 'dazzling', 'debate', 'debris', 'debug', 'debuggers', 'debugging', 'decide', 'decision', 'decisions', 'decreasing', 'deep', 'deeply', 'defenseman', 'define', 'defined', 'defining', 'definition', 'defunct', 'degrees', 'delineate', 'deliver', 'delivered', 'delivering', 'demo', 'democratized', 'democratizing', 'demonstrate', 'demonstrating', 'demonstrations', 'demos', 'department', 'departure', 'dependencies', 'depends', 'deploy', 'deployed', 'deploying', 'deployment', 'derivative', 'derive', 'describe', 'described', 'describes', 'description', 'design', 'designed', 'designer', 'designers', 'detail', 'detailed', 'details', 'deteciton', 'detect', 'detecting', 'detection', 'detects', 'determine', 'determining', 'deterministic', 'develop', 'developed', 'developer', 'developers', 'developing', 'development', 'device', 'dice', 'dictate', 'dictionaries', 'dictionary', 'did', 'didn', 'difference', 'differences', 'different', 'differentiation', 'difficult', 'diffusion', 'dimensionality', 'dimensions', 'dipy', 'directed', 'direction', 'directions', 'directly', 'disadvantages', 'discerning', 'disciplinary', 'discover', 'discovered', 'discovery', 'discuss', 'discussed', 'discusses', 'discussing', 'discussion', 'disease', 'disk', 'dismod', 'display', 'displayed', 'disputed', 'distant', 'distill', 'distributed', 'distribution', 'distributions', 'districts', 'dive', 'diverse', 'diversity', 'dives', 'divided', 'division', 'django', 'dmri', 'dnns', 'do', 'document', 'documentation', 'does', 'doesn', 'doing', 'dollar', 'dollars', 'domain', 'domains', 'don', 'done', 'double', 'down', 'download', 'downloadable', 'downloads', 'draft', 'dramatically', 'drawback', 'drawn', 'drive', 'driven', 'drivers', 'driving', 'drop', 'due', 'during', 'e', 'each', 'eadvisor', 'early', 'ease', 'easier', 'easiest', 'easily', 'easy', 'eat', 'ec2', 'econometric', 'economic', 'ecosystem', 'ed', 'edge', 'edges', 'education', 'educational', 'educators', 'effect', 'effecting', 'effectively', 'effectiveness', 'effects', 'efficacy', 'efficiency', 'efficient', 'efficiently', 'effort', 'efforts', 'eg', 'either', 'ejected', 'elements', 'elucidate', 'embody', 'emerging', 'empahsis', 'emperical', 'empirical', 'employing', 'en', 'enable', 'enabled', 'enables', 'encodes', 'end', 'energy', 'engagement', 'engaging', 'engine', 'engineer', 'engineering', 'engines', 'english', 'enhance', 'enhancing', 'enough', 'enrolled', 'ensemble', 'ensure', 'entire', 'entirely', 'environment', 'environments', 'equations', 'equivalent', 'eroding', 'escience', 'especially', 'essential', 'established', 'estimate', 'estimates', 'estimating', 'estimation', 'etc', 'ethical', 'evaluate', 'evaluating', 'evaluation', 'even', 'evening', 'event', 'events', 'eventual', 'eventually', 'ever', 'every', 'everybody', 'everyone', 'everywhere', 'evidence', 'exactly', 'examine', 'example', 'examples', 'excellent', 'except', 'exceptionally', 'excited', 'exciting', 'execute', 'executed', 'executing', 'execution', 'exeperimentation', 'exercises', 'exist', 'existing', 'exists', 'expand', 'expands', 'expansion', 'expect', 'expensive', 'experience', 'experienced', 'experiences', 'experiment', 'experimental', 'experimentation', 'experiments', 'expert', 'experts', 'explain', 'explaining', 'explains', 'explanations', 'exploding', 'exploration', 'exploratory', 'explore', 'explored', 'exploring', 'explosion', 'exponential', 'expressions', 'expressive', 'expressiveness', 'extend', 'extended', 'extending', 'extensible', 'extensions', 'external', 'extract', 'extracted', 'extracting', 'extraction', 'extremely', 'face', 'faced', 'faculty', 'fail', 'fails', 'failure', 'failures', 'fairly', 'faith', 'fall', 'familiar', 'familiarity', 'family', 'famous', 'fancy', 'far', 'fashion', 'fast', 'faster', 'fat', 'fault', 'fav', 'favorite', 'fear', 'feature', 'featured', 'features', 'federal', 'feedback', 'fellow', 'few', 'fewer', 'fft', 'fiber', 'field', 'fields', 'figure', 'figures', 'file', 'files', 'filter', 'filtering', 'finally', 'financial', 'find', 'finding', 'findings', 'finds', 'fine', 'finite', 'first', 'fish', 'fit', 'fits', 'fitting', 'fix', 'fixed', 'fixes', 'fixie', 'fizzbuzz', 'flame', 'flawed', 'fledged', 'fleeting', 'flexibility', 'flexible', 'floating', 'flocks', 'flow', 'flux', 'flyers', 'focus', 'focused', 'focuses', 'focusing', 'folk', 'folks', 'follow', 'following', 'foobar', 'food', 'fool', 'for', 'forecast', 'forecasting', 'forecasts', 'forest', 'form', 'formalized', 'formats', 'formed', 'formula', 'forums', 'forward', 'fouls', 'foundation', 'foundational', 'four', 'fourier', 'fragments', 'frame', 'framework', 'frameworks', 'fraud', 'fraudulent', 'fraught', 'free', 'frequently', 'fresh', 'freshman', 'friction', 'friday', 'from', 'front', 'frustrated', 'full', 'fully', 'fun', 'function', 'functional', 'functionality', 'functionally', 'functions', 'fund', 'fundamental', 'fundamentally', 'funders', 'funds', 'further', 'furthermore', 'future', 'g', 'gain', 'game', 'gamergate', 'games', 'gaps', 'gathered', 'gathering', 'gave', 'gay', 'gbd', 'geared', 'genders', 'general', 'generalize', 'generally', 'generate', 'generates', 'generating', 'generation', 'genetics', 'genome', 'genomics', 'gensim', 'geometry', 'get', 'gets', 'getting', 'ggplot', 'giants', 'github', 'give', 'given', 'giving', 'global', 'globally', 'glue', 'go', 'goal', 'goals', 'goes', 'good', 'got', 'government', 'gpgpu', 'gpu', 'gradient', 'grading', 'graduation', 'granular', 'graph', 'graphical', 'graphics', 'graphlab', 'graphs', 'great', 'greater', 'greatly', 'group', 'grouped', 'groups', 'grow', 'growing', 'guidance', 'guide', 'guided', 'guis', 'gushing', 'gwas', 'hack', 'had', 'hadoop', 'half', 'hand', 'handful', 'handle', 'handles', 'handling', 'hands', 'hard', 'hardware', 'harness', 'harnessing', 'has', 'hashtag', 'have', 'having', 'hdfs', 'he', 'head', 'health', 'hear', 'heart', 'help', 'helped', 'helpful', 'here', 'heroku', 'hierarchical', 'high', 'higher', 'highlight', 'highlights', 'highly', 'hinton', 'hints', 'historically', 'history', 'hoc', 'hockey', 'hodrick', 'hold', 'holder', 'homophobic', 'hood', 'hope', 'hopefully', 'host', 'hosted', 'hot', 'hour', 'house', 'how', 'however', 'html', 'http', 'https', 'huge', 'human', 'humans', 'humbling', 'hypothesis', 'hypothetical', 'i', 'ia', 'id3', 'ideal', 'ideally', 'ideas', 'identify', 'identifying', 'if', 'ii', 'il', 'ill', 'illustrate', 'illustrated', 'illustrating', 'iloc', 'image', 'imagenet', 'images', 'imaginary', 'immediate', 'immediately', 'impact', 'impala', 'impart', 'imparted', 'implement', 'implementation', 'implementations', 'implementing', 'implements', 'implications', 'importance', 'important', 'imposed', 'improve', 'improved', 'improving', 'in', 'inadequate', 'include', 'included', 'includes', 'including', 'inclusive', 'income', 'incorrect', 'increase', 'increased', 'increasing', 'increasingly', 'incredibly', 'independently', 'index', 'indexers', 'indexes', 'indexing', 'individual', 'industrial', 'industry', 'infer', 'inference', 'inferences', 'inferred', 'infestation', 'influential', 'inform', 'information', 'informed', 'infractions', 'infrastructure', 'infrastructures', 'ing', 'initial', 'injecting', 'innovative', 'input', 'inputs', 'inside', 'insidious', 'insight', 'insights', 'inspect', 'inspirational', 'inspired', 'install', 'installation', 'installment', 'instance', 'instances', 'instead', 'institute', 'integers', 'integral', 'integrate', 'integrated', 'integrating', 'integration', 'intel', 'intelligence', 'intelligent', 'intelligibly', 'intended', 'intensive', 'intent', 'intentions', 'interacting', 'interaction', 'interactions', 'interactive', 'interactively', 'interest', 'interested', 'interesting', 'interface', 'intermediate', 'internalization', 'internalized', 'internals', 'internet', 'interoperability', 'interpret', 'interpretable', 'interpreting', 'intersecting', 'interweaving', 'intimidating', 'into', 'intractable', 'intrangeset', 'intrinsic', 'introduce', 'introduces', 'introducing', 'introduction', 'intuition', 'intuitions', 'intuitive', 'investigate', 'investigating', 'involved', 'involves', 'involving', 'io', 'ipp', 'ipython', 'ironically', 'is', 'isn', 'issue', 'issues', 'it', 'items', 'iter', 'iterative', 'its', 'itself', 'ix', 'jan', 'java', 'javascript', 'jewel', 'jinja2', 'jit', 'jobs', 'join', 'joined', 'joining', 'js', 'json', 'jumping', 'just', 'k', 'kaggle', 'kanye', 'keep', 'kernel', 'key', 'keynote', 'kit', 'knife', 'know', 'knowing', 'knowledge', 'known', 'kris', 'kwh', 'l', 'lab', 'label', 'lack', 'lacks', 'landscape', 'language', 'languages', 'laptop', 'large', 'larger', 'lasso', 'last', 'later', 'latest', 'launched', 'law', 'layer', 'layman', 'lda', 'lead', 'leaderboards', 'leaders', 'learn', 'learned', 'learning', 'least', 'leave', 'leaving', 'lecture', 'legal', 'lemmatization', 'lend', 'lenet', 'less', 'lessons', 'let', 'letang', 'lets', 'letting', 'level', 'levels', 'leverage', 'leveraged', 'leverages', 'libraries', 'library', 'lie', 'life', 'lift', 'light', 'lightning', 'like', 'limit', 'limitations', 'limited', 'line', 'linear', 'lines', 'linguistic', 'list', 'listed', 'lists', 'literacies', 'literature', 'little', 'live', 'lives', 'living', 'll', 'lmm', 'lms', 'loc', 'local', 'locality', 'locate', 'locations', 'log', 'logged', 'logic', 'logistical', 'logs', 'long', 'longer', 'look', 'looking', 'looks', 'loops', 'lose', 'lot', 'lots', 'low', 'lsst', 'luigi', 'luminosity', 'lunch', 'm', 'machine', 'machines', 'macro', 'macroeconomics', 'made', 'magic', 'mainly', 'maintains', 'major', 'majority', 'make', 'makes', 'making', 'males', 'manage', 'management', 'managing', 'manipulate', 'manipulating', 'many', 'market', 'marketers', 'marketing', 'marketplace', 'marketplaces', 'markov', 'massive', 'match', 'matching', 'material', 'math', 'mathematical', 'mathematics', 'matplotlib', 'matrix', 'matter', 'maturing', 'may', 'maybe', 'mds', 'mean', 'meaningful', 'means', 'meanwhile', 'measure', 'measured', 'measurement', 'measurements', 'measures', 'mechanism', 'media', 'median', 'medical', 'medline', 'meets', 'meetup', 'meetups', 'member', 'memex', 'memory', 'mentors', 'merging', 'mesh', 'meshing', 'messages', 'messy', 'method', 'methodology', 'methods', 'metrics', 'microscopic', 'microsoft', 'microsoftgenomics', 'might', 'million', 'millions', 'mimic', 'min', 'mind', 'minimization', 'minimize', 'mining', 'minor', 'minority', 'minutes', 'mirrors', 'misogynistic', 'mission', 'missions', 'mistake', 'mistakes', 'mixed', 'mixing', 'mkl', 'ml', 'mllib', 'mnist', 'mode', 'model', 'modeled', 'modeling', 'models', 'modern', 'modified', 'modular', 'modularity', 'module', 'modules', 'monetize', 'money', 'monitor', 'monitoring', 'monte', 'months', 'more', 'morning', 'morton', 'most', 'motivate', 'motivated', 'motivation', 'move', 'moved', 'movement', 'moves', 'moving', 'mpi', 'mpld3', 'mri', 'mscompbio', 'much', 'multi', 'multicore', 'multiindex', 'multiindexes', 'multinode', 'multiple', 'multithreading', 'munging', 'mushroom', 'music', 'mvps', 'my', 'mystery', 'n', 'naive', 'name', 'named', 'native', 'natural', 'naturally', 'nature', 'navigate', 'ndarray', 'near', 'nearest', 'necessary', 'need', 'needed', 'needs', 'neighborhoods', 'neighbors', 'nerve', 'net', 'netflix', 'nets', 'network', 'networks', 'neural', 'never', 'new', 'newer', 'newest', 'next', 'nhl', 'nirvana', 'nlp', 'nltk', 'no', 'node', 'nodes', 'non', 'nonparametric', 'noon', 'normal', 'not', 'notebook', 'notebooks', 'notes', 'notoriously', 'novel', 'now', 'nowhere', 'numba', 'number', 'numbers', 'numeric', 'numerical', 'numerous', 'numpy', 'nuts', 'nutshell', 'o', 'object', 'objects', 'obligatory', 'observable', 'observational', 'observe', 'observed', 'obtain', 'obtained', 'occurred', 'occurrences', 'octree', 'odds', 'odo', 'of', 'off', 'offer', 'offered', 'offerings', 'offers', 'offs', 'often', 'old', 'on', 'once', 'one', 'oneself', 'ongoing', 'online', 'only', 'only16', 'open', 'operate', 'operation', 'operational', 'operations', 'opinion', 'opportunity', 'optimal', 'optimization', 'optimize', 'optimized', 'optimizing', 'options', 'or', 'orbital', 'order', 'ordering', 'org', 'organizations', 'organize', 'organized', 'organizers', 'oriented', 'original', 'originally', 'ospc', 'other', 'others', 'otherwise', 'our', 'out', 'outbreaks', 'outcome', 'outcomes', 'outline', 'output', 'outputs', 'outputting', 'outreach', 'outside', 'over', 'overall', 'overcome', 'override', 'overtime', 'overview', 'overwhelmed', 'overwhelming', 'owing', 'own', 'owners', 'p', 'pace', 'package', 'packages', 'pagerank', 'paint', 'panda', 'pandas', 'pandastic', 'parallel', 'parallelism', 'parallelization', 'parallelize', 'parallelizing', 'parameters', 'parametric', 'parquet', 'parse', 'parsed', 'parser', 'parsing', 'part', 'participants', 'participated', 'particle', 'particular', 'particulars', 'partnered', 'parts', 'past', 'path', 'paths', 'patterns', 'pay', 'payments', 'peek', 'peer', 'peers', 'penalties', 'penalty', 'penguins', 'people', 'per', 'perceptron', 'perfect', 'perfectly', 'perform', 'performance', 'performant', 'performed', 'performing', 'perhaps', 'period', 'periodic', 'permitting', 'perspective', 'pervasive', 'pet', 'petabytes', 'pharmaceuticals', 'phase', 'phenomena', 'phenomenology', 'phenotype', 'philadelphia', 'physical', 'physics', 'picture', 'pieces', 'pip', 'pipeline', 'pipelines', 'pitfalls', 'pla', 'place', 'places', 'plane', 'platform', 'platforms', 'play', 'players', 'please', 'plink', 'plot', 'plotly', 'plots', 'plotting', 'point', 'points', 'poised', 'policy', 'policymakers', 'poor', 'popular', 'portion', 'pos', 'posed', 'poses', 'position', 'positioning', 'possibilities', 'possible', 'possibly', 'post', 'postgres', 'potential', 'potentially', 'power', 'powerful', 'practical', 'practicality', 'practice', 'pre', 'precisely', 'precision', 'predict', 'prediction', 'predictions', 'predictive', 'preferences', 'preliminary', 'premises', 'prepares', 'prescott', 'present', 'presentations', 'presented', 'presenting', 'presents', 'previous', 'previously', 'prices', 'primarily', 'primary', 'primer', 'primitives', 'principles', 'prior', 'priori', 'privacy', 'probabalistic', 'probabilities', 'probability', 'probe', 'probing', 'problem', 'problems', 'process', 'processes', 'processing', 'produce', 'produces', 'product', 'production', 'productive', 'products', 'professional', 'profile', 'profiler', 'profiles', 'program', 'programmers', 'programming', 'programs', 'progress', 'project', 'projections', 'projects', 'proper', 'properties', 'property', 'proposed', 'pros', 'prospect', 'prototype', 'prototypes', 'prototyping', 'prove', 'proven', 'provide', 'provided', 'provides', 'providing', 'proxy', 'pstreader', 'public', 'publication', 'publications', 'publishers', 'pubmed', 'purdue', 'pure', 'purposes', 'pushing', 'put', 'puts', 'puzzles', 'pycppad', 'pydata', 'pylearn', 'pymb', 'pymc', 'pysnptools', 'pyspark', 'python', 'pythonic', 'q', 'quality', 'quantified', 'quantify', 'quantile', 'quantitatively', 'query', 'question', 'questionable', 'questions', 'queue', 'quick', 'quickly', 'quocnet', 'r', 'races', 'racist', 'rainfall', 'raise', 'raising', 'ramp', 'random', 'randomization', 'randomly', 'randomness', 'range', 'ranging', 'ranking', 'rapid', 'rapidly', 'rarely', 'rate', 'rates', 'rather', 'ratio', 'raw', 're', 'reach', 'read', 'readily', 'reading', 'ready', 'real', 'realistic', 'realized', 'really', 'reason', 'recall', 'recaps', 'recasting', 'receive', 'recent', 'recognition', 'recognized', 'recommend', 'recommendation', 'recommendations', 'recommender', 'recover', 'recruit', 'recurrent', 'recurring', 'recursion', 'redesigned', 'redmond', 'redshift', 'reduce', 'reduced', 'reducing', 'reduction', 'references', 'referred', 'refine', 'reflect', 'reflection', 'reforms', 'refunded', 'regarding', 'regions', 'registration', 'regression', 'regressor', 'regular', 'regularities', 'regulation', 'reinforcement', 'related', 'relation', 'relationships', 'release', 'released', 'relevant', 'relies', 'reordered', 'repeatable', 'repeatably', 'replacement', 'replicate', 'reports', 'represented', 'reproducing', 'repurpose', 'request', 'requires', 'requiring', 'resampling', 'research', 'researchers', 'reshaping', 'resident', 'residual', 'resource', 'resources', 'respond', 'response', 'responses', 'responsibility', 'rest', 'result', 'resulting', 'results', 'retention', 'retrieve', 'retweeting', 'reveal', 'revenue', 'review', 'reviews', 'revolutionary', 'revolutionizing', 'revolves', 'rewrite', 'rewritten', 'ridership', 'riemann', 'right', 'rinaldo', 'rinchiera', 'rise', 'risk', 'roam', 'robust', 'robustness', 'rocket', 'roi', 'role', 'roles', 'room', 'root', 'round', 'rpy2', 'rule', 'rules', 'run', 'running', 'runs', 'runtime', 's', 'said', 'sal', 'same', 'sameness', 'sampler', 'samplers', 'samples', 'sampling', 'sandtraps', 'satellites', 'saturday', 'saturdays', 'save', 'saving', 'savings', 'say', 'scaffold', 'scaffolding', 'scala', 'scalability', 'scalable', 'scale', 'scales', 'scenarios', 'scenes', 'scheduled', 'scheduling', 'school', 'schools', 'sci', 'science', 'sciences', 'scientific', 'scientist', 'scientists', 'scikit', 'scipy', 'score', 'scorecards', 'scored', 'scoring', 'scrape', 'scraping', 'scratch', 'scripts', 'scrubs', 'seaborn', 'seamless', 'seamlessly', 'search', 'searches', 'season', 'seasonal', 'seasonality', 'seasons', 'seattle', 'second', 'section', 'see', 'seek', 'seeks', 'seen', 'segmenting', 'segregate', 'selecting', 'selection', 'self', 'selling', 'semantics', 'sensors', 'sentences', 'sentiment', 'separated', 'separates', 'sequoia', 'serialization', 'series', 'serve', 'server', 'serves', 'service', 'services', 'session', 'sessions', 'set', 'sets', 'setting', 'settings', 'settingwithcopy', 'setup', 'several', 'sframe', 'sgraph', 'shape', 'share', 'shareability', 'sharing', 'she', 'shed', 'shelf', 'ships', 'shockingly', 'shootout', 'short', 'should', 'shouldn', 'shout', 'show', 'showcase', 'showcases', 'shown', 'shows', 'si', 'side', 'signal', 'signals', 'significant', 'similar', 'similarities', 'similarity', 'simple', 'simpler', 'simplest', 'simplified', 'simply', 'simulate', 'simulated', 'simulating', 'simulation', 'simulations', 'since', 'single', 'site', 'size', 'skill', 'skynet', 'slice', 'slicing', 'slideshow', 'slope', 'slow', 'slowly', 'small', 'smaller', 'smallest', 'smooth', 'smoothing', 'snacks', 'sncosmo', 'so', 'social', 'software', 'solely', 'solution', 'solutionism', 'solutions', 'solve', 'solving', 'some', 'someone', 'something', 'sometimes', 'somewhat', 'soon', 'sophisticated', 'sort', 'soundly', 'source', 'sourced', 'sources', 'space', 'spark', 'sparkling', 'sparse', 'spatially', 'speak', 'specialized', 'specific', 'specifically', 'specification', 'specified', 'specify', 'specifying', 'speech', 'speed', 'spent', 'split', 'sponsor', 'spotlight', 'spring', 'spun', 'spyre', 'sql', 'stack', 'stage', 'stages', 'stampede', 'standard', 'standardizable', 'standardization', 'standardized', 'stars', 'start', 'started', 'starting', 'starts', 'stata', 'state', 'states', 'static', 'statistical', 'statistics', 'statuses', 'staying', 'stemming', 'step', 'stephanie', 'stepping', 'steps', 'stick', 'sticking', 'still', 'stitch', 'stock', 'stop', 'storage', 'store', 'stored', 'storing', 'story', 'straight', 'strategies', 'strategy', 'streaming', 'streamline', 'streams', 'strength', 'strengths', 'strings', 'stripe', 'structure', 'structured', 'structures', 'student', 'students', 'studied', 'studies', 'studio', 'studying', 'style', 'subarray', 'subsequent', 'subset', 'subsetting', 'substantial', 'substantially', 'succeed', 'success', 'successes', 'successful', 'successfully', 'such', 'sufficient', 'suitable', 'suite', 'summarization', 'sunday', 'supernova', 'supernovae', 'supervised', 'supervision', 'support', 'supported', 'supports', 'sure', 'surprising', 'survey', 'surveys', 'suspended', 'sustainable', 'swam', 'swamp', 'swarm', 'swelling', 'swiss', 'symbolic', 'symptoms', 'sympy', 'synergize', 'syntax', 'synthetic', 'system', 'systemic', 'systems', 't', 'tables', 'tagging', 'tail', 'take', 'taken', 'takes', 'taking', 'talent', 'talk', 'talks', 'tandem', 'target', 'targeted', 'task', 'tasks', 'taught', 'tax', 'taxbrain', 'taxonomy', 'teach', 'teaching', 'team', 'teams', 'teaser', 'tech', 'technical', 'technique', 'techniques', 'technologies', 'technology', 'tell', 'tells', 'temperature', 'tend', 'tendencies', 'terabytes', 'term', 'test', 'tested', 'testing', 'tests', 'text', 'textblob', 'textbook', 'textual', 'tfidf', 'than', 'thanks', 'that', 'the', 'theano', 'their', 'them', 'themselves', 'then', 'theory', 'there', 'thereby', 'therefore', 'these', 'they', 'things', 'think', 'thinkbox', 'thinking', 'this', 'those', 'though', 'thousand', 'thousands', 'threaded', 'three', 'through', 'throughout', 'throughput', 'thus', 'tidy', 'tied', 'tigres', 'time', 'timeline', 'times', 'tips', 'tire', 'tissue', 'title', 'tmb', 'to', 'today', 'together', 'tolerant', 'too', 'took', 'tool', 'toolchain', 'toolkit', 'tools', 'toolz', 'top', 'topic', 'topics', 'tossing', 'total', 'touch', 'towards', 'track', 'tracked', 'trade', 'tradeoffs', 'traditional', 'train', 'trained', 'training', 'trajectories', 'transactions', 'transform', 'transformation', 'transformations', 'transformed', 'translate', 'translating', 'translation', 'transparent', 'tree', 'trees', 'tremendous', 'trend', 'trends', 'tricks', 'tries', 'trip', 'trivial', 'troll', 'trolls', 'trouble', 'trustworthy', 'try', 'trying', 'tuned', 'tuples', 'turn', 'turned', 'tutorial', 'tweak', 'tweeting', 'twitter', 'twittersphere', 'two', 'type', 'types', 'typically', 'tzeng', 'u', 'ubiquitous', 'uci', 'ui', 'um', 'unable', 'unbiased', 'unconditional', 'under', 'underlay', 'underlying', 'understand', 'understanding', 'unfamiliar', 'unfortunately', 'uniform', 'unique', 'uniquely', 'unit', 'units', 'universe', 'universities', 'university', 'unparalleled', 'unseen', 'unstructured', 'unsupervised', 'unsupported', 'untapped', 'unused', 'up', 'updates', 'us', 'usability', 'usable', 'usage', 'use', 'used', 'useful', 'usefulness', 'user', 'users', 'uses', 'using', 'utilities', 'utility', 'utilize', 'uxers', 'v1', 'v2', 'vacation', 'valid', 'validate', 'validating', 'validation', 'value', 'variables', 'variance', 'variety', 'various', 'vast', 'vb', 've', 'vectorization', 'vectors', 'vehicle', 'vehicles', 'velocity', 'vernacular', 'version', 'versus', 'very', 'via', 'video', 'vision', 'visual', 'visualization', 'visualizations', 'visualize', 'visualized', 'visualizer', 'viz', 'volume', 'volumes', 'volunteers', 'voyage', 'vs', 'w', 'wa', 'wade', 'walk', 'want', 'wants', 'war', 'warehouse', 'warnings', 'was', 'washington', 'water', 'wave', 'way', 'waypoints', 'ways', 'we', 'wear', 'weave', 'weaves', 'web', 'webgl', 'webscraping', 'website', 'websites', 'weight', 'well', 'were', 'what', 'whatever', 'when', 'where', 'whet', 'whether', 'which', 'while', 'white', 'who', 'whole', 'whom', 'whose', 'why', 'wide', 'wifi', 'wikipedia', 'wildly', 'will', 'win', 'windowing', 'wisdom', 'wish', 'with', 'within', 'without', 'woman', 'word', 'word2vec', 'wordcount', 'words', 'work', 'workers', 'workflow', 'workflows', 'working', 'workloads', 'works', 'workshop', 'workshops', 'world', 'worrying', 'worst', 'worth', 'would', 'wrangle', 'wrap', 'write', 'writes', 'writing', 'written', 'wrong', 'xpatterns', 'xray', 'year', 'years', 'yes', 'yet', 'yield', 'you', 'your', 'zac', 'zero', 'zeroth']\n",
      "Answer6: 'a' has the highest score.\n"
     ]
    }
   ],
   "source": [
    "#Question6: Rank each unique word by their TF-IDF score. Which one has the highest score? Use only the highest for each word.\n",
    "#So if a word appears twice only use the highest score\n",
    "\n",
    "string = ''\n",
    "for w in sorted(d, key=d.get, reverse=True):\n",
    "    string = string + w + \" \" + str(d[w]) + \" \\n\"\n",
    "\n",
    "print sorted(d.keys())\n",
    "\n",
    "#Answer6: 'a' has the highest score\n",
    "\n",
    "print \"Answer6: 'a' has the highest score.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer7: Efficient ways to remove space ‘ ‘ are by using Regex or NLTK word tokenizer. For my code I removed space, is not there because I used NLTK word_tokenizer to extract just words.\n"
     ]
    }
   ],
   "source": [
    "#Question7: Is the space ‘ ‘ still there? As one of the words. If so how can you remove it?\n",
    "\n",
    "#Answer7: space '' can be removed by using regex or NLTK tokenizer (better way). For my code it is not there because I used NLTK\n",
    "#word_tokenizer to extract just words.\n",
    "\n",
    "print \"Answer7: Efficient ways to remove space ‘ ‘ are by using Regex or NLTK word tokenizer. For my code I removed space, is not there because I used NLTK word_tokenizer to extract just words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SectionA Output: \n",
      "[('data', 231), ('python', 122), ('time', 113)]\n",
      "\n",
      "SectionB Output: \n",
      "a 2.41828045096 \n",
      "t 2.11622924559 \n",
      "e 1.72299795568 \n",
      "i 1.69794256372 \n",
      "s 1.52936929776 \n",
      "o 1.51291006541 \n",
      "c 1.50337387688 \n",
      "k 1.46229905964 \n",
      "n 1.43889410758 \n",
      "r 1.39314038167 \n",
      "g 1.30739766625 \n",
      "as 1.16322048573 \n",
      "eg 1.10686550146 \n",
      "m 1.09626187805 \n",
      "l 1.07855005689 \n",
      "d 0.971568693448 \n",
      "on 0.96698600106 \n",
      "u 0.963435081189 \n",
      "h 0.95636455166 \n",
      "p 0.93276881687 \n",
      "at 0.890351759449 \n",
      "f 0.884333806697 \n",
      "in 0.845240655364 \n",
      "is 0.753555104133 \n",
      "and 0.750688774878 \n",
      "y 0.699717780632 \n",
      "an 0.664532096739 \n",
      "b 0.649208048956 \n",
      "it 0.630602814906 \n",
      "or 0.560565365361 \n",
      "w 0.552067233157 \n",
      "so 0.549128454822 \n",
      "x 0.517602895561 \n",
      "me 0.512354873408 \n",
      "to 0.500167402565 \n",
      "he 0.49865764998 \n",
      "v 0.483941243371 \n",
      "co 0.445933008951 \n",
      "not 0.440571125686 \n",
      "the 0.429307221365 \n",
      "of 0.415166590733 \n",
      "you 0.409052675281 \n",
      "do 0.367331788025 \n",
      "be 0.345049534018 \n",
      "ie 0.313839374797 \n",
      "z 0.311983992887 \n",
      "us 0.298643648467 \n",
      "if 0.297457403049 \n",
      "too 0.274082671181 \n",
      "we 0.272956089813 \n",
      "his 0.267706162735 \n",
      "per 0.264862181599 \n",
      "every 0.264186962164 \n",
      "for 0.258008401579 \n",
      "are 0.247943216747 \n",
      "even 0.235650657285 \n",
      "very 0.235025236483 \n",
      "your 0.232465862943 \n",
      "who 0.221249734133 \n",
      "ever 0.218697829125 \n",
      "all 0.218508660775 \n",
      "one 0.213072608672 \n",
      "what 0.211419790698 \n",
      "how 0.206520187547 \n",
      "our 0.20452633764 \n",
      "some 0.202540074672 \n",
      "q 0.198178117661 \n",
      "found 0.197242475872 \n",
      "will 0.196896083887 \n",
      "ten 0.186232690043 \n",
      "they 0.184084133231 \n",
      "once 0.179950682803 \n",
      "over 0.177447613073 \n",
      "off 0.177225552341 \n",
      "from 0.175625564571 \n",
      "alone 0.173598242988 \n",
      "its 0.172671602314 \n",
      "by 0.170458086938 \n",
      "has 0.168950754217 \n",
      "into 0.16731722957 \n",
      "using 0.164520586028 \n",
      "yes 0.161736527679 \n",
      "these 0.161499935354 \n",
      "this 0.157697255073 \n",
      "but 0.15578856776 \n",
      "being 0.15518574653 \n",
      "then 0.153305024647 \n",
      "four 0.153065329656 \n",
      "which 0.151992368504 \n",
      "about 0.151824893499 \n",
      "her 0.150827169157 \n",
      "now 0.149802684388 \n",
      "make 0.149481674929 \n",
      "should 0.149450567074 \n",
      "like 0.148717130957 \n",
      "out 0.145826965858 \n",
      "that 0.144405395802 \n",
      "other 0.143568754677 \n",
      "made 0.141835865153 \n",
      "through 0.139460557056 \n",
      "let 0.139365315602 \n",
      "up 0.1389888346 \n",
      "have 0.137838770869 \n",
      "whom 0.137114568234 \n",
      "yet 0.135694696484 \n",
      "used 0.132695998715 \n",
      "around 0.128603002911 \n",
      "my 0.126362452636 \n",
      "their 0.126142351926 \n",
      "along 0.123119507232 \n",
      "several 0.122873230791 \n",
      "miss 0.122809102754 \n",
      "another 0.120262498396 \n",
      "stop 0.115732161992 \n",
      "they're 0.115732161992 \n",
      "can 0.113266095765 \n",
      "she 0.113013033259 \n",
      "can't 0.11251783716 \n",
      "could 0.112301787369 \n",
      "many 0.112087925305 \n",
      "three 0.111633324349 \n",
      "where 0.110891212091 \n",
      "would 0.108509122584 \n",
      "each 0.108090118158 \n",
      "when 0.105853469399 \n",
      "might 0.105505745084 \n",
      "was 0.104916713032 \n",
      "j 0.10346260558 \n",
      "also 0.102010049012 \n",
      "any 0.101188134222 \n",
      "across 0.101026633983 \n",
      "here 0.100460548318 \n",
      "nowhere 0.0987641447152 \n",
      "don't 0.0965412046522 \n",
      "nor 0.0965412046522 \n",
      "well 0.0948930688483 \n",
      "etc 0.0931509858803 \n",
      "often 0.0897456440585 \n",
      "ours 0.0892933491232 \n",
      "same 0.0886068536806 \n",
      "two 0.0866865599441 \n",
      "only 0.0858192721309 \n",
      "because 0.0855836806729 \n",
      "though 0.083450871818 \n",
      "such 0.0824717485219 \n",
      "them 0.081353089084 \n",
      "why 0.0805994121856 \n",
      "you've 0.0797295822696 \n",
      "you'll 0.0797295822696 \n",
      "everywhere 0.079560005465 \n",
      "we'll 0.0794677902142 \n",
      "own 0.0770094625096 \n",
      "beginning 0.0747464833778 \n",
      "anyone 0.0745951587099 \n",
      "than 0.0730798935839 \n",
      "million 0.0719307534604 \n",
      "doesn't 0.0702239220223 \n",
      "already 0.0699329612905 \n",
      "had 0.0692913744897 \n",
      "we're 0.0681233772557 \n",
      "they've 0.0676949283421 \n",
      "begin 0.0671172313731 \n",
      "taking 0.066441318558 \n",
      "while 0.0659747038108 \n",
      "otherwise 0.0658427631434 \n",
      "makes 0.0651448333226 \n",
      "isn't 0.0644520575233 \n",
      "still 0.0639384475203 \n",
      "behind 0.0635847455114 \n",
      "does 0.0635742321714 \n",
      "first 0.0635698312081 \n",
      "rather 0.0635014372059 \n",
      "been 0.0634538446597 \n",
      "before 0.0629849711119 \n",
      "together 0.0620613945354 \n",
      "against 0.0620241227443 \n",
      "between 0.0602240506182 \n",
      "instead 0.0601312491978 \n",
      "there 0.0598614156506 \n",
      "whole 0.0596700040987 \n",
      "something 0.0595288994155 \n",
      "itself 0.0595288994155 \n",
      "via 0.0594120733088 \n",
      "you're 0.0593608330931 \n",
      "during 0.0593608330931 \n",
      "others 0.057866080996 \n",
      "those 0.0572511795031 \n",
      "actually 0.0557693969243 \n",
      "it's 0.0550123065616 \n",
      "overall 0.0548164630955 \n",
      "throughout 0.0548164630955 \n",
      "becoming 0.0527622235608 \n",
      "down 0.0524497209679 \n",
      "hers 0.0523865879625 \n",
      "billion 0.0512875149715 \n",
      "we've 0.0507711962566 \n",
      "thousand 0.0470021554865 \n",
      "further 0.0465717105161 \n",
      "however 0.0452315654947 \n",
      "much 0.0439753119032 \n",
      "after 0.043627204761 \n",
      "themselves 0.0413597140323 \n",
      "whether 0.0413597140323 \n",
      "hereby 0.0409165742391 \n",
      "thereby 0.0409165742391 \n",
      "whatever 0.0409165742391 \n",
      "thus 0.0409165742391 \n",
      "next 0.0388573776404 \n",
      "since 0.0388573776404 \n",
      "whose 0.0384410485943 \n",
      "recent 0.0375034620432 \n",
      "either 0.0375034620432 \n",
      "among 0.0360906916317 \n",
      "aren't 0.036027172286 \n",
      "maybe 0.036027172286 \n",
      "according 0.0358020024592 \n",
      "become 0.0348912682951 \n",
      "becomes 0.0340616886278 \n",
      "shouldn't 0.0337954005515 \n",
      "became 0.0337954005515 \n",
      "didn't 0.0315609939035 \n",
      "toward 0.0315609939035 \n",
      "towards 0.0315609939035 \n",
      "were 0.0305546563371 \n",
      "sometimes 0.0301507378019 \n",
      "sometime 0.0301507378019 \n",
      "did 0.0296514975383 \n",
      "someone 0.0268305404847 \n",
      "eight 0.0268305404847 \n",
      "always 0.0237689642883 \n",
      "perhaps 0.0237689642883 \n",
      "whoever 0 \n",
      "seemed 0 \n",
      "yourselves 0 \n",
      "thereupon 0 \n",
      "i'll 0 \n",
      "you'd 0 \n",
      "likely 0 \n",
      "mr 0 \n",
      "thirty 0 \n",
      "wherever 0 \n",
      "i'd 0 \n",
      "weren't 0 \n",
      "i'm 0 \n",
      "ninety 0 \n",
      "yourself 0 \n",
      "therefore 0 \n",
      "thru 0 \n",
      "until 0 \n",
      "herein 0 \n",
      "who'll 0 \n",
      "must 0 \n",
      "none 0 \n",
      "anywhere 0 \n",
      "nine 0 \n",
      "six 0 \n",
      "eighty 0 \n",
      "hereupon 0 \n",
      "whenever 0 \n",
      "that's 0 \n",
      "indeed 0 \n",
      "he's 0 \n",
      "somewhere 0 \n",
      "he'd 0 \n",
      "haven't 0 \n",
      "hereafter 0 \n",
      "whither 0 \n",
      "everyone 0 \n",
      "we'd 0 \n",
      "beyond 0 \n",
      "forty 0 \n",
      "they'll 0 \n",
      "one's 0 \n",
      "what'll 0 \n",
      "whereupon 0 \n",
      "besides 0 \n",
      "anyhow 0 \n",
      "seventy 0 \n",
      "caption 0 \n",
      "ltd 0 \n",
      "hence 0 \n",
      "onto 0 \n",
      "seeming 0 \n",
      "thereafter 0 \n",
      "twenty 0 \n",
      "trillion 0 \n",
      "hundred 0 \n",
      "nobody 0 \n",
      "that'll 0 \n",
      "herself 0 \n",
      "here's 0 \n",
      "beforehand 0 \n",
      "seem 0 \n",
      "she'd 0 \n",
      "where's 0 \n",
      "nothing 0 \n",
      "noone 0 \n",
      "mrs 0 \n",
      "fifty 0 \n",
      "won't 0 \n",
      "there've 0 \n",
      "meanwhile 0 \n",
      "they'd 0 \n",
      "him 0 \n",
      "what've 0 \n",
      "hasn't 0 \n",
      "namely 0 \n",
      "that've 0 \n",
      "there'd 0 \n",
      "there's 0 \n",
      "co. 0 \n",
      "recently 0 \n",
      "thence 0 \n",
      "afterwards 0 \n",
      "seems 0 \n",
      "couldn't 0 \n",
      "moreover 0 \n",
      "meantime 0 \n",
      "there're 0 \n",
      "what's 0 \n",
      "else 0 \n",
      "former 0 \n",
      "myself 0 \n",
      "wouldn't 0 \n",
      "seven 0 \n",
      "whereafter 0 \n",
      "almost 0 \n",
      "wherein 0 \n",
      "beside 0 \n",
      "upon 0 \n",
      "he'll 0 \n",
      "whereby 0 \n",
      "nevertheless 0 \n",
      "yours 0 \n",
      "inc. 0 \n",
      "she'll 0 \n",
      "adj 0 \n",
      "who's 0 \n",
      "five 0 \n",
      "who'd 0 \n",
      "whomever 0 \n",
      "therein 0 \n",
      "somehow 0 \n",
      "ourselves 0 \n",
      "there'll 0 \n",
      "everything 0 \n",
      "although 0 \n",
      "sixty 0 \n",
      "anything 0 \n",
      "whence 0 \n",
      "formerly 0 \n",
      "himself 0 \n",
      "elsewhere 0 \n",
      "i've 0 \n",
      "amongst 0 \n",
      "wasn't 0 \n",
      "nonetheless 0 \n",
      "whereas 0 \n",
      "let's 0 \n",
      "she's 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question8: Paste the output for both Section A and Section B\n",
    "\n",
    "print \"SectionA Output: \\n\", (nltk.FreqDist(cleaned_text)).most_common(3)\n",
    "print \"\\nSectionB Output: \\n\", sB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
